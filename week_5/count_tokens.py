"""
Write functions to accomplish the following:
1. Fill in tokenize_file
2. Fill in count_tokens_in_file
3. Run program with your favorite text

@author: Alexandra DeLucia
"""
import re


# 1. Define a function to split long text into a word list
# and sanitize for analysis
# "words" are called "tokens"
def tokenize_file(file):
    # Open the file

    
    # Clean all symbols from the text
    # Hint: how did we clean the text for our palindrome method?
    # Remove symbols

    # This added extra spaces. Replace multiple spaces with a single space
    
    # Split content into word array
    # Because of our space/newline replacment, all words are
    # separated by a single space
    
    # Convert words to lower case
    
    return


# 2. Define a function to count tokens
def count_tokens_in_file(file):
    # Convert file into a list of words using our 
    # tokenize_file method
    
    # Create a data structure to hold tokens and their counts
    # Which data structure would work best for this?
    
    # Loop through the token list and update token counts
    
    return


# Separate our main from the rest of the file
if __name__ == "__main__":
    # 1. Get the raw token counts
    token_counts_dict = count_tokens_in_file("")
    
    # 2. Answer the following questions
    
    # How many unique words are in the text?
    
    # What is the most popular word?
    
    
    
    
    